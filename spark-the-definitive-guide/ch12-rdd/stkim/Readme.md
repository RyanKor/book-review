# RDD

- 스파크의 저수준 API: RDD, SparkContext, Accumulator, broadcast variable과 같은 분산형 공유 변수 등을 의미함.

## 1. 저수준 API란

- 분산 데이터 처리의 RDD

- 공유 변수를 배포하고 다루기 위한 API인 브로드캐스트 & 어큐물레이터

### 1.1 저수준 API는 언제 사용할까

- 고수준 API에서 제공하지 않는 기능을 필요로 할 때 : 아주 세밀한 데이터 배치 등

- RDD를 사용해 개발된 기존 코드 유지 & 보수

- 사용자 정의 공유 변수 다룰 때

- 스파크에 대해 잘 이해하고 있는 개발자라도 구조적 API 기반으로 사용하는 게 좋음.

    - 저수준 API 를 사용해야하는 예시: 이전 버전 스파크에서 자체 구현한 파티셔너를 사용해 데이터 파이프라인 실행 동안 변수 값 갱신하고 추적하는 저수준 API 필요.

### 1.2 저수준 API는 어떻게 사용할까?

- SparkContext는 저수준 API 기능 사용을 위한 진입 지점임.
- SparkSession이 SparkContext로 접근해서 사용함.

## 2. RDD 개요

- RDD는 스파크 1.x의 핵심 API였다. (그럼 2.x, 3.x는?)
- Dataset, DataFrame -> RDD로 컴파일
- 불변성을 갖고, 병렬로 처리할 수 있는 파티셔닝된 레코드 모음.
- RDD 모든 객체는 자바 또는 파이썬 객체로 완벽하게 제어 가능.
- RDD로 UDF 구현 가능하지만 어마어마한 수작업을 요구 (바퀴를 다시 발명하지 마라)


### 2.1 RDD 유형

- 사용자는 Generic RDD or key-value RDD 두 개 타입 RDD 구현 가능

- RDD의 다섯 가지 주요 속성

    - 파티션 목록
    - 각 조각을 연산하는 함수
    - 다른 RDD와의 의존성 목록
    - 부가적으로 key-value RDD를 위한 Partitioner
    - 부가적으로 각 조각 연산을 위한 기본 위치 목록

- RDD에는 Row 타입이 없음. 그래서 구조적 API에서 제공하는 여러 기능을 사용 못하기에 상당한 성능 저하 발생.

### 2.2 RDD는 언제 사용할까

- 저자의 당부 : 꼭 필요한 경우 (필요한 데이터 타입이 없다던지...)가 아니면 쓰지 마라.

- 왠만해선 DataFrame으로 커버 된다.

### 2.3 Dataset과 RDD의 케이스 클래스

- 왠만해선 Dataset을 써라. 최적화, 데이터 타입이 다 정의 되어 있다.

## 3. RDD 생성하기

### 3.1 DataFrame, Dataset으로 RDD 생성하기

- 파이썬은 Row 타입 RDD만 생성 가능

### 3.2 로컬 컬렉션으로 RDD 생성하기

- 컬렉션 객체를 RDD로 호출하려면 SparkContext의 parallelize 메서드 호출해야함.

### 3.3 데이터소스로 RDD 생성하기

- 구조적 API 사용하면 DataSource API를 호출해 사용하겠지만, RDD를 사용하면 DataSource API 개념 자체가 없음.

## 4. RDD 다루기

- RDD 자체가 스파크 데이터 타입 대신 자바나 스칼라 객체 다루는 것이 가장 큰 특징.

## 5. 트랜스포메이션

- 트랜스포메이션을 이용해 사용자만의 새 RDD 생성 가능함.

### 5.1 distinct

- `distinct` 메서드 호출 가능

### 5.2 filter

- `filter` 메서드 호출 가능

### 5.3 map

- `map`: 주어진 입력을 원하는 값으로 빤환하는 함수 명시하고 레코드 별로 적용함.

### 5.4 flatMap

- map 메서드 확장 버전. 단일 로우를 여러 로우로 변환하는 경우 등에 사용 가능함.

### 5.5 sortBy

- RDD의 데이터 객체에서 값을 추출해서 값을 기준으로 정렬.

### 5.6 randomSplit

- RDD를 임의로 분할해 RDD 배열을 만들 때 사용.

## 6. 액션

- 데이터를 드라이버로 모으거나 외부 데이터 소스로 보낼 때 사용.

### 6.1 reduce

- 모든 값을 하나로 만들 때.
- 예를 들어, 2개의 입력값을 하나로 줄일 때

### 6.2 count

- countApprox : RDD 전체 Row 수의 근사치를 제한된 시간 내 계산. 시간이 부족하면 값의 부정확도가 커짐.
- countApproxDistinct : 최신 카디널리티 추정 알고리즘 엔지니어링
- countByValue : RDD 값의 개수 구함.
- countByValueApprox : `반드시 지정된 제한 시간 내에` 근사치 계산.

### 6.3 first

- 데이터셋 첫번재 값 반환

### 6.4 max & min

- 최대, 최소값 반환.

### 6.5 take

- 하나의 파티션을 먼저 읽고, 파라미터로 지정된 값을 만족하는 추가 파티션 예측.

## 7. 파일 저장하기

- 고수준 API 내부 처리 과정을 저수준 API로 구현하는 접근법.

### 7.1 saveAsTextFile

- 압축 코덱 설정 가능하고, 하둡에서 사용 가능한 코덱 임포트.

### 7.2 시퀀스 파일

- 바이너리 키-값 플랫 파일로, 맵리듀스 입출력 포맷으로 사용.

### 7.3 하둡 파일

- 스파크 자체가 하둡 생태계 지원하므로 하둡 파일 포맷 호환.

## 8. 캐싱

- 캐싱 및 저장 가능.

## 9. 체크 포인팅

- RDD를 디스크에 저장하는 방식인데, DataFrame API에서 미지원.

## 10. RDD를 시스템 명령으로 전송하기

- RDD를 외부 프로세스로 전달 가능한 기능, pipe
- 여러 줄의 데이터로 변환해서 프로세스 표준 입력으로 전달.
- 결과 파티션에서 프로세스 표준 출력으로 생성.

### 10.1 mapPartitions

- 파티션 단위로 동작하는 map 함수
- 개별 파티션에 대해 map 연산 수행할 수 있음.
- 전체 파티션에 대해서 수행하는 것도 가능함.

### 10.2 foreachPartition

- 반환 값이 없음.
- 반환 값 존재 여부가 mapPartitions와의 차이임.
- 파티션을 DB에 저장하는 행위 등과 같이 개별 파티션에서 특정 작업을 수행하는데 매우 적합함.

### 10.3 

- 모든 파티션을 배열로 반환.