# 구조적 API 기본 연산

## 들어가기 전에 : Pandas vs Spark DataFrame

- Python Pandas를 사용할 줄 안다면, 내용이 많이 겹치는 것을 알 수 있다.

- 그렇다면 Pandas와 Spark DataFrame의 차이는 무엇인가?

1. 데이터 처리 용량
- pandas: 주로 로컬 메모리에 데이터를 로드해서 작업하기 때문에 단일 머신의 메모리 크기에 따라 데이터 처리량이 제한됩니다. 대개 수백 MB에서 수 GB 정도의 데이터를 효율적으로 처리할 수 있지만, 그 이상의 대용량 데이터에는 한계가 있습니다.

- Spark DataFrame: 분산 처리 프레임워크인 Spark 위에서 실행되므로, 클러스터의 노드들이 데이터를 분산하여 처리합니다. 따라서, 수십 GB에서 TB 단위의 데이터도 효율적으로 다룰 수 있습니다.

2. 연산 방식
- pandas: 대부분의 연산이 CPU에 의해 즉시 수행됩니다. 각 연산이 로컬 머신의 메모리에서 바로 실행되기 때문에 대화형으로 데이터 탐색 및 변형 작업을 하는 데 적합합니다.

- Spark DataFrame: Spark에서는 연산이 "지연 평가(lazy evaluation)" 방식으로 처리됩니다. 모든 연산을 즉시 실행하지 않고 DAG(Directed Acyclic Graph) 형태로 모아 두었다가, collect(), show()와 같은 액션을 호출할 때 연산이 실제로 수행됩니다. 이 방식은 최적화에 유리합니다.

3. 병렬 처리와 분산 처리
- pandas: 단일 프로세스로 동작하며, 병렬 처리나 분산 처리가 기본적으로 지원되지 않습니다. 여러 코어를 활용하려면 Dask 등의 라이브러리를 추가로 사용해야 합니다.

- Spark DataFrame: 분산 컴퓨팅을 기반으로 하여 다수의 머신과 코어에서 병렬로 작업을 수행합니다. Spark 클러스터 내에서 각 파티션이 병렬로 작업을 나눠서 처리하므로 대용량 데이터에서 성능이 우수합니다.

4. 데이터 소스 통합

- pandas: 주로 CSV, Excel, SQL 데이터베이스 등 비교적 소규모의 데이터 소스에서 데이터를 불러오는 데 최적화되어 있습니다.

- Spark DataFrame: HDFS, AWS S3, Cassandra, HBase 등 다양한 분산 파일 시스템 및 대규모 데이터 소스와의 통합이 잘 이루어져 있습니다. Spark는 이런 대용량 데이터 소스로부터 직접 데이터를 읽고 쓸 수 있기 때문에, 빅데이터 환경에서 활용도가 높습니다.