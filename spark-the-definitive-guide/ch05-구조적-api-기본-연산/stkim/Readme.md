# 구조적 API 기본 연산

## 들어가기 전에 : Pandas vs Spark DataFrame (FEAT. ChatGPT)

- Python Pandas를 사용할 줄 안다면, 내용이 많이 겹치는 것을 알 수 있다.

- 그렇다면 Pandas와 Spark DataFrame의 차이는 무엇인가?

- 책 앞부분에 짤막하게 언급되는데 DataFrame의 파티셔닝은 DataFrame & Dataset이 물리적으로 배치되는 형태를 의미함. (파티셔닝 스키마는 파티션을 배치하는 방법을 정의)

1. 데이터 처리 용량
- pandas: 주로 로컬 메모리에 데이터를 로드해서 작업하기 때문에 단일 머신의 메모리 크기에 따라 데이터 처리량이 제한됩니다. 대개 수백 MB에서 수 GB 정도의 데이터를 효율적으로 처리할 수 있지만, 그 이상의 대용량 데이터에는 한계가 있습니다.

- Spark DataFrame: 분산 처리 프레임워크인 Spark 위에서 실행되므로, 클러스터의 노드들이 데이터를 분산하여 처리합니다. 따라서, 수십 GB에서 TB 단위의 데이터도 효율적으로 다룰 수 있습니다.

2. 연산 방식
- pandas: 대부분의 연산이 CPU에 의해 즉시 수행됩니다. 각 연산이 로컬 머신의 메모리에서 바로 실행되기 때문에 대화형으로 데이터 탐색 및 변형 작업을 하는 데 적합합니다.

- Spark DataFrame: Spark에서는 연산이 "지연 평가(lazy evaluation)" 방식으로 처리됩니다. 모든 연산을 즉시 실행하지 않고 DAG(Directed Acyclic Graph) 형태로 모아 두었다가, collect(), show()와 같은 액션을 호출할 때 연산이 실제로 수행됩니다. 이 방식은 최적화에 유리합니다.

3. 병렬 처리와 분산 처리
- pandas: 단일 프로세스로 동작하며, 병렬 처리나 분산 처리가 기본적으로 지원되지 않습니다. 여러 코어를 활용하려면 Dask 등의 라이브러리를 추가로 사용해야 합니다.

- Spark DataFrame: 분산 컴퓨팅을 기반으로 하여 다수의 머신과 코어에서 병렬로 작업을 수행합니다. Spark 클러스터 내에서 각 파티션이 병렬로 작업을 나눠서 처리하므로 대용량 데이터에서 성능이 우수합니다.

4. 데이터 소스 통합

- pandas: 주로 CSV, Excel, SQL 데이터베이스 등 비교적 소규모의 데이터 소스에서 데이터를 불러오는 데 최적화되어 있습니다.

- Spark DataFrame: HDFS, AWS S3, Cassandra, HBase 등 다양한 분산 파일 시스템 및 대규모 데이터 소스와의 통합이 잘 이루어져 있습니다. Spark는 이런 대용량 데이터 소스로부터 직접 데이터를 읽고 쓸 수 있기 때문에, 빅데이터 환경에서 활용도가 높습니다.

## 5.1 스키마

- 데이터 프레임의 칼럼과 데이터 타입 정의 (스키마의 데이터 타입은 `StructType`)

- 스파크에서 스키마 보는 방법 (그래도 골자는 판다스와 엇비슷)

```python
spark.read.format("json").load("/mnt/nvme/dataset/spark/flight-data/json/2015-summary.json").schema
```

## 5.2 컬럼과 표현식

- 컬럼 : 특별히 컬럼에 대한 정의는 찾아보기 어렵지만, 4장에서 언급하길 다양한 데이터 타입을 가져와 사용할 수 있다고만 언급

- 표현식 : DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합

## 5.3 레코드와 로우

- (다른 프레임워크 또는 라이브러리에서는 모르겠으나) 스파크에서 각 Row는 하나의 레코드임.

## 5.4 DataFrame의 트랜스포메이션

- 아래 작업들이 DataFrame에 대한 트랜스포메이션의 전부임.

    - 로우 & 컬럼 추가 및 제거

    - 로우 <-> 칼럼 사이 변환

    - 컬럼 값 기준 로우 순서 변경

