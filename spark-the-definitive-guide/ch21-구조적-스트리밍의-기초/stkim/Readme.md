# Chapter 21. 구조적 스트리밍의 기초


#### **21.1 구조적 스트리밍의 기초**
- **구조적 스트리밍**은 **스파크 SQL 엔진**을 기반으로 한 스트림 처리 프레임워크.
- **스파크의 구조적 API**(DataFrame, Dataset, SQL)를 활용하여 스트림 데이터를 배치 처리와 동일한 방식으로 다룰 수 있음.
- 배치 연산과 스트림 연산을 일관성 있게 표현하며, 사용자는 **비즈니스 로직만 정의**하면 구조적 스트리밍 엔진이 신규 데이터에 대한 질의 및 연산을 수행.
- 기존 배치 처리와 마찬가지로 **카탈리스트 엔진**을 활용해 **최적화**를 수행.
- **WAL (Write-Ahead Log)**을 통해 내구성을 제공.

#### **21.2 핵심 개념**
- 정확한 비대칭 데이터 처리
   - 스트리밍 데이터 처리는 **배치 처리와 다르지만, 가능한 한 일관된 방식으로 설계**.
   - 전통적인 빅데이터 스트리밍 프레임워크보다 **간결한 API**를 제공하여 사용자 접근성을 높임.

- **트랜스포메이션과 액션**
   - 구조적 스트리밍은 기존 스파크의 트랜스포메이션(변환)과 액션(실행) 개념을 그대로 적용.
   - 다만, 스트림 처리에서는 일부 제한 사항이 존재하며, 일부 쿼리는 즉시 실행되지 않고 **일정한 주기로 연산 수행**.

- **입력 소스**
   - 스트리밍 데이터를 읽을 수 있는 다양한 입력 소스를 지원.
   - 일반적인 파일 소스뿐만 아니라 **Kafka, Socket, S3, HDFS** 등을 활용 가능.


#### **21.2.3 싱크 (Sink)**
- 스트림 데이터의 **출력 목적지를 정의**하는 개념.
- **싱크(Sink)**는 데이터의 흐름을 제어하고 결과를 저장하는 역할.
- 지원되는 싱크:
  - **Apache Kafka 0.10**
  - **HDFS나 S3 같은 분산 파일 시스템**
  - **테스트용 콘솔 출력**
  - **대량의 메모리 싱크**
  - **foreach 싱크** (출력 데이터를 특정 방식으로 변환 가능)

#### **21.2.4 출력 모드 (Output Mode)**
- **데이터를 어떻게 출력할 것인지 결정하는 방식.**
- **스파크가 지원하는 출력 모드:**
  - **append**: 새로운 데이터만 추가
  - **update**: 변경된 데이터만 갱신
  - **complete**: 전체 출력을 매번 재작성

- 특정 쿼리는 특정 출력 모드만 지원.  
  - 예: **집계 연산**은 전체 데이터를 매번 갱신해야 하므로 `complete` 모드가 적합.
  - 반면, 데이터가 지속적으로 입력되는 경우 `append`가 적합.

#### **21.2.5 트리거 (Trigger)**
- **데이터를 언제 출력할지 결정하는 메커니즘**.
- 기본적으로 **최신 입력 데이터를 확인하고 연산을 수행한 후 출력**.
- **트리거 유형:**
  - 기본적으로는 **고정된 시간마다 실행 (micro-batch)**.
  - 향후 더 다양한 방식의 트리거 지원 예정.

#### **21.2.6 이벤트 시간 처리**
- **이벤트 발생 시간 기준으로 데이터를 처리**하는 기능 지원.
- **무작위로 도착하는 데이터에 대해서도 타임스탬프 기반으로 정렬 및 분석 가능**.
- 다음 장에서 구체적인 처리 방식 설명 예정.

#### **이벤트 시간 데이터 처리**
- **이벤트 시간(event-time)**: 데이터가 생성된 시간 기준으로 처리하는 방식.
- 데이터가 **늦게 도착해도 올바르게 처리할 수 있도록 이벤트 시간을 활용**.
- **표준 SQL 연산자와 함께 사용 가능**, 결과 일관성 유지.

#### **워터마크 (Watermark)**
- **이벤트 시간 기반의 데이터 유효 기간을 제한하는 기법**.
- **늦게 도착하는 데이터의 허용 범위를 설정**하여 처리할지 무시할지 결정.
- 예를 들어, **이벤트 시간이 특정 윈도우(시간 범위)를 초과할 경우 더 이상 반영하지 않음**.

---
21.3 구조적 스트리밍 활용 & 21.4 스트림 트랜스포메이션은 예제 설명이므로 생략.
---

#### **21.5 입력과 출력**
- **입력 소스 및 싱크**
  - **Kafka, 파일, 테스트 및 메모리 소스 지원**.
  - **출력도 동일한 방식으로 작동** (Kafka로 쓰거나 파일에 저장 가능).
  - 최근 추가된 소스는 **스파크 공식 문서 확인 필요**.

#### **21.5.1 데이터를 읽고 쓰는 장소 (소스와 싱크)**
- **구조적 스트리밍에서 지원하는 주요 소스와 싱크:**
  1. **파일 소스와 싱크**:
     - CSV, JSON, 파케이(Parquet) 등 파일 기반 데이터 활용.
     - 스파크는 **새로운 파일이 추가될 때만 스트리밍 처리**.
     - **HDFS나 S3 같은 분산 파일 시스템을 활용**.
  2. **카프카 소스와 싱크**:
     - **Apache Kafka**는 데이터 스트림을 **발행-구독 방식**으로 처리하는 시스템.
     - **Kafka의 로그는 내구성을 제공**하며, 토픽(topic)별로 메시지를 저장.
     - **데이터는 오프셋(offset) 관리로 구독 가능**, **publish-subscribe 패턴**을 따름.
     - 스파크는 **Kafka 데이터를 배치 & 스트리밍 방식으로 읽을 수 있음**.

#### **21.5.2 카프카 소스에서 메시지 읽기**
- **카프카에서 메시지를 읽는 방법**
  1. **assign**: 특정 파티션을 직접 지정하여 메시지 읽기.
  2. **subscribe**: 특정 토픽을 지정하여 구독.
  3. **subscribePattern**: 정규 표현식을 사용해 특정 패턴에 맞는 토픽을 구독.

- **카프카 서버 정보 지정 필요**:  
  - `.option("kafka.bootstrap.servers", "host1:port1,host2:port2")`

#### **추가 옵션**
1. **startingOffsets 및 endingOffsets**  
   - **earliest**: 가능한 가장 오래된 메시지부터 읽기.  
   - **latest**: 가장 최신 메시지부터 읽기.  
   - **특정 오프셋 지정 가능** (JSON 형식 예시: `{ "topicA": {"0":23, "1":7}, "topicB": {"0":-2}}`).
   - **쿼리 실행마다 오프셋이 초기화됨**. **일정한 오프셋 유지 필요 시 체크 필요**.

2. **failOnDataLoss**  
   - 메시지 손실이 발생할 경우 **쿼리를 중단할지 여부** (`true` 또는 `false`).

3. **maxOffsetsPerTrigger**  
   - 한 번의 트리거 실행에서 읽을 최대 오프셋 개수 지정 가능.

### **카프카 소스의 데이터 스키마**
- **카프카에서 읽은 데이터는 아래 스키마를 가짐:**
  - **키 (key):** binary
  - **값 (value):** binary
  - **토픽 (topic):** string
  - **파티션 (partition):** int
  - **오프셋 (offset):** long
  - **타임스탬프 (timestamp):** long

- **카프카의 메시지는 다양한 방식으로 처리 가능**:
  - **JSON이나 Avro 형식**을 활용하는 것이 일반적.


### **21.5.3 카프카 싱크에 메시지 쓰기**
- **카프카로 메시지를 스트리밍 방식으로 출력하는 방법**

- **Python 코드 예시:**
  ```python
  df1.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)") \
    .writeStream.format("kafka") \
    .option("checkpointLocation", "/to/HDFS-compatible/dir") \
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
    .start()

  df1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
    .writeStream.format("kafka") \
    .option("checkpointLocation", "/to/HDFS-compatible/dir") \
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
    .option("topic", "topic1") \
    .start()
  ```

- **필수 옵션:**
  - `.option("kafka.bootstrap.servers", "host1:port1,host2:port2")` : Kafka 서버 정보 지정.
  - `.option("checkpointLocation", "/to/HDFS-compatible/dir")` : 체크포인트 저장 경로.

### **foreach 싱크**
- **foreach 싱크란?**
  - **foreachPartitions와 유사**하게, 입력 데이터에 대해 커스텀 연산 수행 가능.
  - 기본적으로 **각 파티션에서 실행됨**.
  - 특정 서비스나 외부 저장소로 데이터를 전송할 때 활용.

- **foreach 싱크를 사용하려면 `ForeachWriter` 인터페이스 구현 필요**
  - `open()`, `process()`, `close()` 세 가지 메서드 필수.
  - `open()`: 각 파티션이 시작될 때 호출.
  - `process()`: 각 레코드마다 호출됨.
  - `close()`: 작업 종료 시 호출.

- **구현 방식 주의점**
  - `ForeachWriter`의 `writer`는 **UDF나 함수처럼 반드시 `Serializable` 인터페이스를 구현**해야 함.
  - **연결을 매번 초기화하면 성능에 문제 발생** (가능하면 연결을 재사용).
  - 구조적 스트리밍은 실패 시 데이터 손실이 발생할 수도 있음 → **체크포인트 설정 필수**.

#### **ForeachWriter 개요**
- **ForeachWriter**는 `foreach` 싱크를 사용할 때 구현해야 하는 인터페이스.
- **구성 요소**:
  1. **open(partitionId: Long, version: Long)**  
     - 스트림의 **각 파티션별로 실행**되며, 데이터 저장을 위한 **초기화 과정**.
     - `true`를 반환하면 `process` 메서드 실행, `false`이면 실행되지 않음.

  2. **process(record: String)**  
     - 각 **레코드를 처리**하는 메서드.
     - 데이터를 저장하거나 원하는 작업을 수행.

  3. **close(errorOrNull: Throwable)**  
     - **마지막 정리 작업**을 수행하는 메서드.
     - 오류 발생 시 예외를 받아 처리.


### **테스트용 소스와 싱크**
- **스파크는 여러 가지 테스트용 소스와 싱크를 제공**.
- **운영 환경에서는 사용 금지**, 주로 **개발 및 디버깅 용도**.

#### **소켓 소스**
- **TCP 소켓을 통해 데이터 스트리밍 입력** 가능.
- 실행 시 **새로운 TCP 연결 생성**.
- **예제 코드**
  ```python
  socketDF = spark.readStream.format("socket") \
    .option("host", "localhost").option("port", 9999).load()
  ```
- **테스트 방법**:
  - Unix/macOS에서는 `NetCat`을 사용하여 9999 포트로 테스트 데이터 전송 가능.
  - 실행 명령:
    ```sh
    nc -lk 9999
    ```
  - 이후 **NetCat 터미널에서 입력한 데이터가 스트리밍 처리됨**.

#### **콘솔 싱크**
- **스트리밍 처리 결과를 콘솔에 출력**.
- **운영 환경에서는 비효율적**이지만, **디버깅 시 유용**.
- 지원하는 출력 모드: `append`, `complete`.

### **21.5.4 데이터 출력 방법 (출력 모드)**  
구조적 스트리밍에서 **출력 모드(output mode)**를 사용하여 데이터가 싱크로 저장되는 방식을 결정함.  
세 가지 주요 출력 모드:
1. **append 모드**
2. **complete 모드**
3. **update 모드**

### **append 모드**
- 가장 기본적인 방식이며, 새로운 행만 추가.
- **과거 데이터 변경 없음** → **트랜잭션 로그 기반 저장소에 적합**.
- **비내구성 저장소 (예: 메모리)**와 함께 사용.
- **이벤트 시간과 워터마크**를 설정하여 중복 데이터 방지 가능.

### **complete 모드**
- **출력 데이터 전체를 싱크에 저장**.
- 모든 테이블이 지속적으로 변경될 경우 적합.
- 일부 **상태 기반 데이터(stateful data)**에서 유용.
- **비교적 높은 리소스 사용량** 발생 가능.

#### **update 모드**
- **이전 실행 결과 중 변경된 부분만 출력** (complete 모드와 유사).
- **업데이트 가능한 싱크에서만 사용 가능**.
- 집계 연산을 하지 않는 경우 **append 모드와 동일**.

#### **출력 모드 선택 가이드**
- **map 연산만 수행하는 경우** → `complete` 모드 사용 가능.
- **이벤트 시간 처리 및 워터마크 필요** → `append` 모드 권장.
- **상태가 유지되는 연산 (stateful operations)** → `update` 모드 필요.


#### **추가 정보**
- **메모리 싱크 (Memory Sink)**
  - 콘솔 싱크와 유사하지만 **데이터를 메모리에 유지**.
  - **운영 환경에서는 사용 불가** (데이터 지속성 없음).
  - 개발 및 디버깅 시 유용.
  - 예제 코드:
    ```scala
    activityCounts.writeStream.format("memory").queryName("my_device_table")
    ```

  ### **요약: 스파크 구조적 스트리밍 - 트리거, Dataset API, 정리**

---

### **21.5.5 데이터 출력 시점 (트리거)**
- **트리거(Trigger)**는 **데이터를 싱크로 출력하는 시점을 제어**하는 기능.
- **기본적으로는 직접 트리거가 실행될 때마다 즉시 데이터 출력**.
- **트리거 사용 이유**:
  - **출력 부하 방지**: 너무 많은 데이터가 빠르게 쌓이는 경우 제한 필요.
  - **출력 크기 조절**: 처리 주기를 조절하여 부담 완화.

#### **트리거 유형**
- **처리 시간 기반 트리거 (ProcessingTime)**
   - **특정 주기마다 실행**.
   - 예제 코드:
     ```python
     activityCounts.writeStream.trigger(processingTime="5 seconds")\
       .format("console").outputMode("complete").start()
     ```

- **일회성 트리거 (One-Time)**
   - **한 번만 실행 후 종료** (Batch 작업처럼 동작).
   - 주로 개발 및 디버깅 용도로 사용.
   - 예제 코드:
     ```python
     activityCounts.writeStream.trigger(once=True)\
       .format("console").outputMode("complete").start()
     ```


### **21.6 스트리밍 Dataset API**
- 구조적 스트리밍은 기본적으로 **DataFrame API**를 사용하지만, **Dataset API도 지원**.
- **Dataset API 특징**:
  - **타입 안정성 제공** → **Scala의 case 클래스 및 Java 빈 클래스 활용 가능**.
  - **정적 DataFrame과 동일한 방식으로 동작**.

- **Dataset을 활용하면 정적 데이터와 동일한 연산 수행 가능**.
- **Stream API에서도 Dataset을 활용해 타입 안정성을 확보 가능**.
