# 4. 구조적 API: DataFrame, SQL, Dataset

## 들어가기 전에 (1): Spark 2.x vs 3.x

- 3.0 이상으로 버전 업그레이드가 되면서 SQL 쿼리 분석 속도가 느렸던 것이 많이 개선됨 (단, Trino를 사용하는 것보다 이점이 있는지는 확인 불가.)

- Pandas API 호환성 지원

- GPU 가속화 지원

- MLlib에서 RDD 대신 DataFrame API 사용.

## 들어가기 전에 (2)

- 스파크는 `트랜스포메이션`의 처리 과정을 정의하는 분산 프로그래밍 모델

- 사용자가 정의한 다수의 트랜스포메이션은 DAG로 표현되는 명령 구현.

- 액션은 DAG 처리 프로세스 실행.

## 이해하고 넘어가야할 개념 3개

- 타입형/비타입형 API 개념과 차이점

- 핵심 용어

- 스파크가 구조적 API 데이터 흐름 해석 및 클러스터 실행

## 4.1 DataFrame & Dataset

- DataFrame & Dataset은 결과 생성을 위해 어떤 데이터에 어떤 연산을 적용해야하는지 정의하는 지연 연산의 실행 계획이자 불변성을 지님.

## 4.2 스키마

- DataFrame의 컬럼명과 데이터 타입 정의.

## 4.3 스파크 구조적 데이터 타입 개요

- 카탈리스트 엔진: 스파크 실행 계획 수립 및 처리에 사용하는 자제 데이터 타입 정보 소유.

- 파이썬, R 등으로 구조적 API를 쓰더라도 실제 연산에는 스파크의 데이터 타입을 사용.

### 4.3.1 DataFrame vs Dataset

- DataFrame

    - 비타입형

    - 실제로 타입은 있으나, 런타임 시점에 일치 여부 확인 (즉, 컴파일 타임에 확인 안함)

    - 스파크 DataFrame은 Row 타입으로 구성된 Dataset임.
        - Row는 연산 최적화된 인메모리 포맷의 내부적인 표현 방식.

- Dataset

    - 타입형

    - 컴파일 타임에 확인

    - Scala, Java에서만 확인 (2장 확인)

### 4.3.2 컬럼

- 일반적인 컬럼과 정의 동일

### 4.3.3 로우

- 일반적인 로우와 정의 동일

### 4.3.4 데이터 타입

- 스파크를 `프로그래밍 언어`라고 정의하는만큼, 정의하는 타입이 일반적인 프로그래밍 언어에서 정의하는 타입들과 거의 동일.

## 4.4 구조적 API 실행 과정

- DataFrame/Dataset/SQL 코드 작성

- 논리적 실행 계획으로 변환

- 물리적 실행 계획 변환 후, 최적화 가능 여부 체크

- 물리적 실행 계획 (RDD) 실행

![카탈리스트 옵티마이저](https://github.com/user-attachments/assets/813cddf1-4392-47fa-bcae-831e2a6974d7)

### 4.4.1 논리적 실행 계획

![논리적 실행 계획 수립 과정](https://github.com/user-attachments/assets/53393c7e-db21-463c-9364-b9d6edad5b4a)

- 추상적 트랜스포메이션만 표현

- 드라이버 & 익시큐터 정보 고려 X

- 검증 전 논리적 실행 계획: 코드 유효성, 테이블 & 컬럼 존재 여부 판단.

- 스파크 분석기: 테이블 & 컬럼 존재 여부 검증을 위해 카탈로그, 모든 테이블 저장소 및 DataFrame 정보 활용.

- 옵티마이저: 테이블 & 컬럼 존재 검증 결과 전달받음.

### 4.4.2 물리적 실행 계획

- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의

![물리적 실행 계획](https://github.com/user-attachments/assets/de3130b6-79d9-4412-93bf-2a49c98fb3ac)

- DataFrame/Dataset/SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일.

### 4.4.3 실행

- RDD를 대상으로 모든 코드 실행.