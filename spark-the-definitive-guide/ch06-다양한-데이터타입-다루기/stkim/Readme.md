# 다양한 데이터 타입 다루기

## 들어가기 전에 1: 사이드 프로젝트 목록

- [스파크 사이드 프로젝트 이슈 링크](https://github.com/RyanKor/book-review/issues/3)

- 생각보다 구할 수 있는 데이터는 많다.

- 고민해 보고 싶은 부분은 이슈에 언급되어 있는 프로젝트들이 "반드시" 스파크에서 해보는 것이 의미를 갖느냐는 부분이다.

## 들어가기 전에 2: 스파크는 처리해야할 데이터가 기하급수적으로 늘어날 때 어떻게 처리할까?

- 단순하게 고민해보기로는 스파크도 클러스터 단위로 운영되니까, 처리할 데이터가 클러스터에서 처리할 용량을 넘어서면 노드를 더 붙여줄 것이다.

- 현실에서 `O(logN)`을 보긴 힘들겠지? (처리할 데이터가 많아질 수록 속도가 빨라지는)

6장에 대한 더 자세한 설명은 예제 코드를 보면서 설명 (사실 이 챕터도 스파크 데이터 타입 변환에 대한 기본 개념 설명이라 소스코드 보는 게 더 편함)

## 스파크 UDF 처리 프로세스

- 파이썬이나 스칼라를 사용해서 외부 라이브러리 등을 트랜스포메이션에 사용할 수 있도록 지원하는 방법

- 단, 스파크 본연의 기능은 아니다보니 객체 생성 등이 많으면 성능 저하됨 (19장에서 성능 향상 방법을 확인할 예정)

- 함수를 개발한 언어에 따라 동작 방식이 갈림. 예를 들어, 자바 & 스칼라에서 작업한 UDF는 JVM에서만 동작

- 파이썬은 데이터 직렬화 (파이썬이 이해할 수 있도록) -> 데이터 로우마다 UDF 함수 실행 -> JVM & 스파크에 처리 결과 반환.

![spark udf](https://github.com/user-attachments/assets/4cf85990-ef41-4994-9e63-ab2c93579b46)
