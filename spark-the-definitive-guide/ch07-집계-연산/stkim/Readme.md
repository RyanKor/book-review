# 집계 연산

- Aggregation이라는 작업 자체가 데이터 분석에 빼놓을 수 없는 작엄임.

- 주의 사항: 어떤 결과를 만들지 정확하게 파악하고 있어야 연산 비용을 아낄 수 있음.

## 1. 계 함수

### count

- 트랜스포메이션이 아닌 연산으로써, 즉시 연산하라고 스파크에 명령을 전달하는 것임.

- 전체 row 수 집계.

- 단 count에 특정 컬럼을 지정하면 null 값을 카운트하지 않음.

### countDistinct

- 전체 레코드가 아닌 고유 레코드 수 카운트할 때 사용. (개별 컬럼 처리할 때 적합)

### approx_count_distinct

- 대규모 데이터셋 다룰 때 정확한 고유 개수를 세는 게 무의미할 수 있어, 근사치 파악할 때 사용하는 함수.

- `최대 추정 오류율(maximum estimation error)` 라는 파라미터가 추가되는데, 오류율 값에 따라 countDistict보다 더 빠른 결과 반환 가능.

- 그래서 대규모 데이터셋에서 집계할 때 종종 사용됨 (실제 레퍼런스에서 자주 사용하는지는 모름)

### first, last

- DataFrame에서 처음 및 마지막 값 조회할 때 사용

### min, max

- DataFrame에서 최소, 최대값 추출할 때 사용

### sum

- 특정 컬럼의 모든 값 합산

### sumDistict

- 고유값에 대한 합산

### avg

- sum 함수의 연산 값을 count로 나누면 평균 값을 구할 수 있지만, 스파크에서는 계산 편의를 위해 avg, mean 함수를 사용해 평균 값 구할 수 있음.

- 집계된 컬럼을 재활용할 땐 alias 메서드를 사용하면 됨.

### 분산 및 표준편차

- 스파크는 표본표준편차, 모표준편차 모두 지원

- variance, stddev -> 표본표준분산과 표본표준편차 공식 사용

- var_pop, stddev_pop -> 모표준분산과 모표준편차 공식 사용

### 비대칭도와 첨도

- 비대칭도 : 데이터의 평균 비대칭 정도 측정

- 첨도 : 데이터 끝 부분 측정

- 비대칭도와 첨도는 확률변수의 확률 분포로 데이터를 모델링할 때 특히 중요함.

### 공분산과 상관관계

- 단일 컬럼 말고 두 컬럼 사이의 영향도를 비교할 때 사용함.

- cov, corr -> 공분산과 상관관계 계산 가능

- 상관관계는 피어슨 상관계수를 측정하고 -1 ~ 1 사이 값 가짐.

### 복잡한 데이터 타입의 집계

- 특정 컬럼을 리스트로 수집하거나 set data type으로 고윳값만 수집할 수 있음.

## 2. 그룹화

- 단일 컬럼 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해 연산하려면 카테고리형 데이터를 사용

- 그룹화 단계: 하나 이상의 컬럼을 그룹화 먼저 수행, 그 후 집계 연산 진행.

### 표현식을 이용한 그룹화

- SQL 사용 때문에 그런지 생각보다 표현식 사용이 잦다.

- agg 메서드 사용해서 여러 집계 처리 한 번에 수행 가능.

- 그리고 agg 메서드에서 집계 수행 시점에 표현식 적용 가능함.

### 맵을 이용한 그룹화

- 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 맵 타입을 사용한 트랜스포메이션

## 3. 윈도우 함수

- 특정 윈도우를 대상으로 고유 집계 연산 수행

- 데이터 윈도우는 현재 데이터에 대한 참조를 사용해 정의

- group-by vs window: 모든 로우 레코드가 단일 그룹으로만 이동. 윈도우 함수는 프레임에 입력되는 모든 로우에 대해 결과값 계산.

![윈도우 함수 시각화](https://github.com/user-attachments/assets/91fec2d3-b12c-4e9b-b383-a74fa72c5759)

- 윈도우 함수 정의하기 위해서 먼저 윈도우 명세를 구성함.

- 이 때 사용하는 partitionBy 메서드는 파티셔닝 스키마와는 전혀 관계 없고 그룹을 어떻게 나눌지 결정하는 것과 유사한 개념임.

- 사실 잘 이해가 안되서 장황하게 설명했는데, chatgpt에 물어보니 아래와 같이 답변해줌.

![window function by chatgpt](https://github.com/user-attachments/assets/6b2d8592-4cfa-4021-b597-a91b23b2aacd)

## 4. 그룹화 셋

- 여러 집계를 결합하는 저수준 기능

- group-by 구문에서 원하는 형태로 집계 결과 생성

- `GROUPING SETS` 구문은 SQL에서만 사용 가능.

- DataFrame으로 표현하려면 rollup & cube 메서드 함께 사용.

### 롤업

- 다차원 집계 기능

- 합치는 컬럼의 row가 모두 null이면, 투 컬럼이 속한 레코드의 전체 합계가 표기됨.

### 큐브

- 롤업의 고차원적 사용 버전

### 그룹화 메타데이터

- cube & rollup 사용하면 집계 수준에 따라 쉽게 필터링하는 작업이 필요하여 집계 수준을 조회하는 경우가 발생함.

- 이 때 grouping_id라는 것을 사용해서 데이터셋 집계 수준 명시하는 컬럼 제공됨.

### 피벗

- 로우 -> 컬럼 변환

## 5. 사용자 정의 집계 함수

- UDAF (User Defined Aggregation Function): 자바와 스칼라에서만 사용 가능. 언급은 없지만 자바 & 스칼라만 사용 가능하다는 것은 Dataset 과 관계 있지 않을까?

- 스파크는 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리함.

- UserDefinedAggregateFunction 상속 받아서 아래 메서드 정의해야함.

![image](https://github.com/user-attachments/assets/17bc5cae-41a0-4aa4-a6ef-70bc562361b5)
