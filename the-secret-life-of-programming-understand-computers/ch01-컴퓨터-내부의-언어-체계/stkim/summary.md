# 1. 언어란 무엇인가

### 언어란

편의를 제공하기 위한 지름길로 복잡한 개념을 직접 보여주지 않고도 의사소통을 가능하게 해주는 도구이다.

### 언어의 구성

언어는 기호의 집합으로 인코딩 된다. 의사소통하는 당사자들이 모두 같은 문맥을 공유해 어떤 기호에 같은 의미를 부여할 수 있어야한다.  
예로 토토(Toto)라는 단어는 오즈의 마법사에 나오는 개를 뜻하기도 하고 열선이 들어간 변좌를 생산하는 일본 기업을 뜻하기도 한다.

    토토(Toto) > 오즈의 마법사에 나오는 개
              > 열선이 들어간 변좌를 생성는 일본 기업
              > 스포츠 토토

    캐미솔(Camisole) > 여성용 민소매 속옷
                   > 죄수에게 입히는 구속복

이를 구분할 수 있는 요소는 문맥 뿐이고 이를 명확히 식별 할 수 있는 것은 아니다. 이와 같은 문제가 컴퓨터 언어에도 똑같이 존재한다.

# 2. 문자 언어

### 문자 언어란

문자 언어란 기호들을 나열한 것이다. 기호를 정해진 순서대로 나열하여 단어를 만든다. 언어에 따라 다양한 조합과 순서로 구성되어 있다.

### 문자 언어의 구성 요소

- 기호가 들어갈 상자
- 상자에 들어갈 기호
- 상자의 순서

![한글 구조 예시](https://user-images.githubusercontent.com/91672778/167243502-e7f201bb-ea07-449d-bffa-c6f66932646e.jpeg)  
위의 세 가지 구성요소가 문자 언어의 틀을 이루고 컴퓨터 언어 역시 마찬가지이다.  
일부 언어에서는 기호의 종류에 따라 들어갈 수 있는 상자가 제한 되는 등의 규칙이 존재하기도 한다.

# 3. 비트

### 비트란

자연어에서의 상자를 문자라고 부르듯이 컴퓨터 언어에서는 이를 `비트`라고 부른다.  
비트라는 단어는 `바이너리(binary)`와 `디지트(digit)`가 합쳐진 단어로 적은 비용으로 편리하게 기호를 담을 수 있다.  
비트는 2진법을 사용해 점과 선처럼 두가지 기호 중 하나만 비트 상자에 담을 수 있다.  
이 비트를 순서에 따라 의미를 부여해 언어로서 사용하게 된다.

    데이터를 나타내는 최소 단위. 모든 데이터는 0과 1의 조합으로 구성되는데, 이 0또는 1이 하나의 비트가 됨.
    1개의 비트는 두 가지 상태를 나타낼 수 있으므로 n개의 비트로는 2ⁿ가지의 상태를 나타낼 수 있음.

# 4. 논리 연산

비트의 사용법 중 하나로 참, 거짓이 담긴 데이터를 하나의 비트로 표현할 수 있다.  
예로 "날씨가 추운가?", "내 모자를 좋아하나?"와 같이 예/아니오로 대답할 수 있는 질문에 대한 답을 표현 할 수 있다.  
하지만 예/아니오로 표현할 수 없는 데이터들은 하나의 비트로 표현 할 수 없다.  
"밖에 비가 내리고 있거나 춥다면 코트를 입어라.", "눈이 오고, 학교가는 날이 아니라면 스키를 타러가라." 와 같이 여러 구절을 묶어 하나의 문장으로 표현 할 수 있다.  
이렇게 다른 비트들이 표현하는 내용으로 부터 새로운 비트를 만들어 내는 동작을 `논리 연산(logic operation)`이라 한다.

### 불리언 대수

대수가 수에 대한 연산 규칙의 집합인 것처럼 `불리언 대수`란 비트에 대해 사용할 수 있는 연산 규칙의 집합이다.  
일반 대수와 마찬가지로 결합 법칙, 교환 법칙, 분배 법칙을 적용할 수 있다.  
불리언 연산자는 기본적으로 `NOT`,`AND`,`OR` 세 가지가 있다.

- NOT: 논리적 반대를 의미한다. 해당 비트의 반대 값을 반환한다.

  | a   | NOT a |
  | --- | ----- |
  | F   | T     |
  | T   | F     |

- AND: 둘 이상의 비트에 작용하여 모든 비트들이 참인 경우에만 참을 반환한다.

  | a   | b   | a AND b |
  | --- | --- | ------- |
  | T   | T   | T       |
  | T   | F   | F       |
  | F   | T   | F       |
  | F   | F   | F       |

- OR: 마찬가지로 둘 이상의 비트에 작용하여 어느 한 비트라도 참인 경우 참을 반환한다.

  | a   | b   | a OR b |
  | --- | --- | ------ |
  | T   | T   | T      |
  | T   | F   | T      |
  | F   | T   | T      |
  | F   | F   | F      |

- XOR: 배타적(exclusive) OR 이라는 연산으로 첫번째 비트와 두번째 비트가 다른 경우에만 참을 반환한다.  
   다른 연산을 이용해 만들 수 있다. a XOR b => (a OR b) AND (NOT(a AND b))

  | a   | b   | a XOR b |
  | --- | --- | ------- |
  | T   | T   | F       |
  | T   | F   | T       |
  | F   | T   | T       |
  | F   | F   | F       |

### 드모르간 법칙

    a AND b = NOT (NOT a OR NOT b)
    a OR b = NOT (NOT a AND NOT b)

NOT 연산을 이용해 AND 연산을 OR 연산으로 혹은 OR 연산을 AND 연산으로 나타낼 수 있다.  
이를 통해 입력에 따라 연산을 최소화해 계산 속도를 높힘과 동시에 비용을 절감할 수 있다.
|춥다|비가 온다|코트를 입는다|
|-|-|-|
|T|T|T|
|T|F|T|
|F|T|T|
|F|F|F|

| NOT 춥다 | NOT 비가 온다 | NOT 코트를 입는다 |
| -------- | ------------- | ----------------- |
| T        | T             | T                 |
| T        | F             | F                 |
| F        | T             | F                 |
| F        | F             | F                 |

# 5. 정수를 비트로 표현하는 방법

앞에서 비트(bit)는 binary와 digit가 합쳐진 말로, 적은 비용으로 편리하게 기호를 담을 수 있는 상자라고 했다. 이제 2진법을 사용하는 이 비트로 특정 수를 표현하는 방법을 알아본다.
<br></br>

### 양의 정수 표현

비트로 정수를 표현하는 방법을 알아보기 전에 우리가 일상 생활에서 사용하는 10진수(Decimal number) 체계부터 알아본다. 10진수 체계에서는 10가지 기호인 숫자(0~9)를 상자에 담을 수 있다. 책의 예시대로 아래 5,028이라는 숫자를 통해 상자에 담는다는 것이 무슨 말인지 표로 나타내보았다.

| 천의 자리 | 백의 자리 | 십의 자리 | 일의 자리 |
| :-------: | :-------: | :-------: | :-------: |
|   10^3    |   10^2    |   10^1    |   10^0    |
|     5     |     0     |     2     |     8     |

`(1000(=10^3) x 5) + (100(=10^2) x 0) + (10(=10^1) x 2) + (1(=10^0) x 8) = 5,028`
5,028은 각 상자 위치에 해당하는 밑(base)의 거듭제곱과 각 자리에 들어간 수를 곱한 값들의 합이다. 10진수를 사용하기 때문에 각 상자의 밑은 10이다. 따라서 상자에 담는다는 것은 각 자리에 대응하는 숫자를 위치시키는 것으로 이해할 수 있다.

이와 같은 방식을 통해 비트로도 수를 표현할 수 있다. 10진수 체계에서는 0부터 9까지, 즉 10 아래의 수들을 사용할 수 있었다면, 비트는 2진수 체계이기 때문에 각 자리 또는 상자에 들어갈 수 있는 수는 0과 1로 두 가지뿐이다. 이외에는 10진수 체계로 수를 표현한 것과 동일하다. 각 상자 위치를 표현하는 밑(base)이 10이 아닌 `2`인 것이다. 그리고 10진수에서는 어떤 자리의 수가 9를 넘어가면 10을 추가로 제곱한 자리(좌측 상자)가 추가되지만, 2진수에서는 1이 넘어가면 추가된다.
|...|2^5|2^4|2^3|2^2|2^1|2^0|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|

<br></br>

그렇다면 5,028을 10진수 대신 2진수로 어떻게 나타낼까.
2진수 변환 결과부터 내보자면 `1001110100100`이다. 10진수에서 각 자리 밑의 거듭제곱과 수를 곱한 값처럼 2진수도 동일하게 고려할 수 있다. 0으로 표현된 자리를 제외하고 유효한 값만 식으로 풀어쓴다면,
`(2^12 x 1) + (2^9 x 1) + (2^8 x 1) + (2^7 x 1) + (2^5 x 1) + (2^2 x 1)`로 나타낼 수 있다. 이 식을 계산하면 `4,096 + 512 + 256 + 128 + 32 + 4`로 5,028이 동일하게 도출된다.
|1|0|0|1|1|1|0|1|0|0|1|0|0|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|

10진수에서는 5,028이 4자리 숫자이지만, 이처럼 2진수로 동일한 값을 표현하면 13자리 숫자(13비트 수)가 된다. 2진수 비트로 표현된 수에서 가장 오른쪽(가장 작은 자릿수)의 비트를 `LSB(Least Significant Bit)`, 반대로 가장 왼쪽(가장 큰 자릿수)의 비트를 `MSB(Most Significant Bit)`라고 부른다. 가장 오른쪽의 비트를 변경하면 값이 가장 작게 변경되고, 가장 왼쪽의 비트를 변경하면 가장 크게 변하기 때문이라고 한다.
<br></br>

|  0  |  0  |  0  |  1  |  0  |  0  |  1  |  1  |  1  |  0  |  1  |  0  |  0  |  1  |  0  |  0  |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |

이 숫자는 동일하게 5,028을 2진법으로 나타냈지만 13자리가 아닌 16자리의 수다. 다시 상기해보자면 비트라는 것은 수가 아니라 수를 담는 상자라고 했다. 따라서 16비트로 표현된 5,028이며 여기서 MSB는 가장 왼쪽에 0이 들어있는 상자고, LSB는 가장 오른쪽에 0이 들어있는 상자이다. 이처럼 MSB 쪽에 추가된 0들을 `리딩 제로(leading zero)`라고 한다.
컴퓨터는 미리 정해진 수의 비트를 한 덩어리로 사용하도록 만들어졌기 때문에 2진수 체계에서 항상 일정한 갯수의 비트를 사용해 값을 표현하는 경우가 있다. 따라서 만약에 16비트로 사용해서 5,028을 표현해야 하는 경우라면 사람이 봤을 때 불필요해 보여도 이처럼 리딩 제로로 16비트를 모두 채우는 것이다.
<br></br>

### 2진수 덧셈

우리가 학교에 다니면서 10진법으로 모든 수학 문제를 풀었듯, 2진수 체계에서도 간단한 산술 연산을 할 수 있다. 덧셈이라면 작은 자리의 수부터 더하고 해당 자릿수를 넘어가면 좌측으로 넘기는 방식으로 동일하다. LSB부터 MSB 쪽으로 더하며 최대 수인 1을 넘으면 그 다음 자리로 넘긴다.

- 10진수: 1 + 5 = 6
- 2진수: 001(=1) + 101(=5) = 110(=6)

<br/>

`AND`와 `XOR` 논리 연산자로 2진수의 각 자리 덧셈을 수행할 수 있다.
`AND`로 동일한 자리의 수를 비교한 결과를 좌측에, `XOR`로 비교한 결과를 우측에 위치시키고 붙여쓴다. 결과가 True라면 `1`을, False라면 `0`으로 표현한다. 예를 들어, 2진수 `1 + 0`을 계산한다면 AND 조건으로 연산한 결과 `0`과 XOR 조건으로 연산한 결과 `1`을 조합해 `01`, 즉 1인 것이다. 두 비트를 서로 더한 결과는 두 비트를 XOR한 값과 같고, 두 비트를 AND한 값은 올림한 것과 같다고 한다.

하지만 덧셈의 결과가 사용할 수 있는 비트 수를 초과할 경우 `오버플로우(overflow)`가 발생한다. 오버플로우는 MSB에서 올림이 발생했다는 뜻이다. 4비트 덧셈 `1001 + 1000`을 수행하는 경우, `10001`이기 때문에 5비트 결과가 나온다. 하지만 추가로 사용할 수 있는 비트가 없다면 결과는 `0001`이 된다. 컴퓨터에는 `조건 코드 레지스터(condition code register)`라는 곳에 `오버플로우 비트`를 담고 있고, 이 비트에는 MSB에서 발생한 올림 값이 들어간다. 이 비트 값을 통해 오버플로우가 발생했는지 여부를 알 수 있다.

`조건 코드 레지스터(Condition code register, CCR)`란, 연산 결과에 따라 CPU에 의해 세트되는 비트들을 저장하는 레지스터라고 한다. 위키피디아에서는 상태 레지스터(Status register) 또는 플래그 레지스터(Flag register)라고도 하며, 프로세서에서의 상태 플래그 비트들의 집합이라고 한다. 여기서 `플래그(Flag)`는, 다른 프로그램에게 약속된 신호를 남기기 위한 용도로 프로그램에서 사용되는 미리 정의된 비트를 의미한다. 또한, 보통 CPU라고 부르는 `프로세서(Processor)`는 제어 장치, 연산 장치, 레지스터, 데이터 버스로 구성된 디지털 시스템의 핵심 부분으로 프로그램을 기억 장치로부터 읽어 연산 처리, 비교 처리, 데이터 전송, 편집, 변환, 테스트와 분기 등의 데이터를 처리하고 각종 장치를 구동하는 역할을 한다.

다시 CCR로 돌아와서 레지스터는 프로세서의 구성 중 하나라고 했는데, 프로세서가 반복적 또는 주기적으로 처리하는 도중의 일시적인 정보는 이 레지스터에 저장해 효율적으로 작동한다. 레지스터는 플립플롭(flip-flop)으로 구성되어 있으며, 레지스터의 여러 종류 중 하나가 조건 코드 레지스터인 것이다.

조건 코드 레지스터는 총 4가지가 있다.

1. `CF(Carry Flag)`: MSB에서 Carry out 또는 Borrow 발생 시 저장
2. `ZF(Zero Flag)`: 결과 값 0인 경우 저장
3. `SF(Sign Flag)`: 부호를 나타내며, 결과 값의 MSB가 1일 경우 저장
4. `OF(Overflow Flag)`: 오버플로우 발생 시 저장

<br></br>

### 음수 표현

이번에는 비트를 사용해서 양수가 아닌 음수를 표현하는 방법이다. 4비트로는 반드시 0부터 15까지 16가지만의 양수만 나타낼 수 있는 것은 아니다.

<br/>

> 부호와 크기

우리는 실생활에서 숫자 앞에 `+` 부호와 `-` 부호를 붙여 양수인지 음수인지 구분한다. 이처럼 비트에서도 MSB에 부호의 역할을 부여해 동일하게 구분하는 방법이 있다.

- +5 표현

|  0  |  1  |  0  |  1  |
| :-: | :-: | :-: | :-: |

- -5 표현

|  1  |  1  |  0  |  1  |
| :-: | :-: | :-: | :-: |

4비트 수 `0101`과 `1101`의 차이는 MSB에 있다. `0`은 `+` 역할로 `0101`은 양수 5를 나타내며, 반대로 `1`은 `-` 역할로 `1101`은 음수 5를 나타낸다. 이처럼 비트 하나를 부호 역할로 사용하고 나머지 비트로 수를 표현하는 방법을 `부호와 크기(Sign and Magnitude) 표현법`이라고 한다.

하지만 이 방법은 두 가지 이유로 잘 활용되지 않는다고 한다.
첫 번째는, 0을 표현하는 방법이 +0과 -0으로 두 가지라서 비용이 낭비된다. 두 값이 정확히 같다고는 할 수 없으며, 같은 개념의 수를 표현하기 위해 비트 하나를 추가로 낭비한다는 뜻이다.
두 번째는, AND와 XOR을 사용한 덧셈 계산을 간편하게 할 수 없다. MSB가 부호 역할을 하는 `0001`과 `1001`의 합은 각각 +1과 -1의 합으로 `0`을 의도한다. 하지만 앞에서 언급한 덧셈 방식인 논리 연산자를 이용한 세로 나열 방식의 덧셈 계산을 하면 `1010`으로 `-2`가 된다. 복잡한 로직을 추가로 구현해 MSB를 부호로, 나머지 부분을 실제 수로 인식하게 해 계산 작업을 수행하면 가능할지는 몰라도 쉬운 계산을 간단하게 하는 방법은 아니다.

<br/>

> 1의 보수

양수의 모든 비트를 1에서 0, 0에서 1로 모두 뒤집어 표현하는 `1의 보수(one's complement) 표현법`이 있다. 부호와 크기 표현법과 비슷하게 비트들을 부호 비트와 나머지 수로 나누어지며, `NOT` 연산을 통해 보수를 얻는다. 각 비트의 수를 서로 반대로 뒤집는 것이다.

- +7 표현

|  0  |  1  |  1  |  1  |
| :-: | :-: | :-: | :-: |

- -7 표현

|  1  |  0  |  0  |  0  |
| :-: | :-: | :-: | :-: |

동일하게 0을 두 가지로 표현한다는 문제가 존재하며, MSB에서 올림이 발생한 경우 다시 LSB로 전달하는 `순환 올림(End-around carry)`을 해야 한다. 이 방법 또한, 순환 올림이 발생한 부분에 대한 계산 과정도 필요하기 때문에 잘 작동하더라도 추가 하드웨어가 필요하다.

<br/>

> 2의 보수

현대 컴퓨터에서는 위의 부호와 크기 표현법과 1의 보수 표현법을 모두 사용하지 않는다고 한다. 이들과 다르게 추가 하드웨어나 비용없이 `AND`와 `XOR` 연산자로 계산할 수 있는 `2의 보수(two's complement) 표현법`이 있다.
2의 보수 표현법에서는 양수 1에 더했을 때 0이 나오도록 하는 숫자 -1을 나타내는 비트 패턴을 찾아 활용한다. 우선 1의 보수 표현법과 동일하게 MSB를 부호로 사용한다. 그럼 4비트 수 0001(= +1)에서 어떤 수를 더해야 0000(= +0)이 나올까. `1111`을 더하면 된다. 하드웨어나 비트를 추가하지 않기 때문에 MSB에서 올림이 발생하면 버리기 때문에 MSB까지 모든 자릿수가 0으로 도출되고 끝난다. 이때 1111을 2의 보수 표기법에서는 `-1`로 정한다. 1의 보수 표기법처럼 단순히 0001을 뒤집으면 1110이 되고, 이 수는 -1이다. 하지만 2의 보수 표기법에서는 여기에 2진수 1을 더한 1111이 -1이 된다.
1 외에도 특정 양수와 같은 절댓값의 음수를 구하려면 0과 1을 서로 뒤집은 다음 2진수 1을 더한다.

- +4

|  0  |  1  |  0  |  0  |
| :-: | :-: | :-: | :-: |

- -4

|  1  |  0  |  1  |  1  |
| :-: | :-: | :-: | :-: |

|  0  |  0  |  0  |  1  |
| :-: | :-: | :-: | :-: |

위 두 2진수를 합한 값이 -4가 된다.

|  1  |  1  |  0  |  0  |
| :-: | :-: | :-: | :-: |

<br/>

2진수 보수법에서는 앞서 다른 방법과 같은 문제가 발생하지 않을까? 위와 동일한 방식으로 +0에 해당하는 0000을 뒤집으면 1111이고, 2진수 1을 더해 올림이 생겨 결국 똑같이 0000이 된다. 따라서 0을 표현하는 방법이 단 하나뿐이다. 이처럼 2의 보수 표현법은 추가 비트나 하드웨어에 드는 비용도 없을 뿐더러 0을 표현하는 방법도 한 가지이다.

<br/>

이처럼 같은 2진수로 이루어진 수라도 표현 방법에 따라 실제 값과 비트 갯수에 따라 표현할 수 있는 값의 범위까지 달라진다.

<br></br>

### 참고 자료

- [Status register(Wikipedia)](https://en.wikipedia.org/wiki/Status_register)
- [cpu의 register](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=jkssleeky&logNo=220380904656)
- [[CSAPP] x86-64 - Control](https://it-eldorado.tistory.com/36)
- [프로세서란?](https://velog.io/@woga1999/%ED%94%84%EB%A1%9C%EC%84%B8%EC%84%9C%EB%9E%80)
- [TCP Flag란](https://hongpossible.tistory.com/entry/TCP-Flag%EB%9E%80)
- [5분컴공 2 / 컴퓨터에서 수를 표현하는 방법 - 정수편](https://miniminis.github.io/2020/07/30/5mincs/cswithpython2/)

# 6. 실수를 표현하는 방법

> ### 고정소수점 표현
>
> 2진수를 사용해 소수를 표현하기 위해 2진 소수점의 위치를 임의로 정한다.
> 그 위치를 일정하게 고정하는 방법이 고정소수점 표현이다
> 쓸모 있는 범위의 실숫값을 표현하기 위해 필요한 비트 개수가 너무 많기 때문에 범용 컴퓨터에서 이런 방식을 사용하는 경우는 드뭄

(단, 디지털 신호 처리 장치 등 특별한 목적에 쓰이는 컴퓨터는 일부 사용)

플랑크 상수(양자 역학 기본 상수 중 하나)는 6.63 _ 10^34J/s(초당 줄)라는 아주 작은 값이지만, 아보가드로 수(Avogadro's constant)는 6.02 _ 10^23/몰 이라는 아주 큰 수다. 두 수의 범위는 10^57에 달하며, 대략 2^191 정도 된다. 거의 200비트가 필요하다. 모든 수를 몇백 비트로 표현하면 메모리 비용이 너무 많이 든다는 단점이 존재한다.

ex) 고정소수점표현 예시
7.625라는 수를 2진수로 표현하면 111.101이고, 이것을 16비트 체계에선 아래와 같이 저장하며,
남는 공간은 0으로 다 채운다.

![고정소수점표현](https://user-images.githubusercontent.com/91880235/167648605-a9f033c0-7e4d-429b-aba6-e0bc31936e5e.png)

> ### 부동소수점 표현법
>
> 플랑크 상수부터 아보가드로 수에 이르는 범위의 값을 2진수로 표현하기 위해 과학적 표기법을 도입하여 큰 범위의 수를 표현함.
> 예를 들어 0.0012라는 실수를 1.2 \* 10^-3이라고 쓰고, 이런 표현법을 부동소수점으로 부른다.
> 하지만 부동소수점도 아래와 같이 비효율적인 부분이 존재한다.

(4비트 예시 기준)

**1. 비트 조합 중에 낭비되는 부분이 많다. 0을 표현하는 방법이 네 가지나 되고 1.0, 2.0, 4.0을 표현하는 방법도 2가지씩 존재**

**2. 비트 패턴이 가능한 모든 수를 표현하지는 못한다. 지수가 커질수록 가수의 한 패턴과 다른 패턴 사이의 값 차이가 커진다.**
<br>

> ### IEEE 부동소수점 수 표준
>
> 이상하지만 부동소수점 수 시스템은 컴퓨터에서 계산을 수행할 때 실수를 표현하는 표준 방법이다. 그래서 낭비되는 비트 조합을 최소화하고 반올림을 쉽게 하기 위한 여러가지 트릭이 사용된다. IEEE 754라는 표준은 이 모든 기능을 정의한다. IEEE는 미국 전자전기공학회의 약자(Insitute Of Electrical and Electronics Engineers)로 표준 제정 등 다양한 활동을 하는 전문가 조직이다.
> <br>

부동소수점은 두 가지 방법이 자주 쓰인다.

첫번 째는 **기본정밀도 부동소수점 수** (float)

두번 째는 **2배 정밀도 부동소수점 수** (double)

두 형태 모두 가수에 대한 부호를 사용한다. 하지만 지수에 대해 부호 비트가 따로 존재하지 않는다. IEEE 754 설계자들은 편향된 지숫값(bias)인을 사용하여 비트 패턴을 활용했다. 기본 정밀도의 경우 편향값은 127이며, 2배 정밀도 수는 편향값을 1023을 사용한다.

IEEE 754에서 편리한 점은 0으로 나눴을 때 생길 수 있는 양의 무한대나 음의 무한대를 표현하는 비트 패턴 등 여러 가지 특별한 비트 패턴을 제공한다. 'NaN'을 표현하는 특별한 값도 존재한다.

똑같은 비트를 사용하더라도 정밀도(precision)를 가능한 한 높이고 싶다면 한 가지 트릭은 정규화(nomalization) 정규화는 가수를 조정해서 맨 앞(즉, 왼쪽)에 0이 없게 만드는 것이다. 이런 식으로 가수를 조정하려면 지수도 조정해야한다. 두번 째 트릭은 디지털 이큅먼트 사(DEC, Digital Equipment Corporation)에서 고안한 것으로, 가수의 맨 왼쪽 비트가 1이라는 사실을 알고 있으므로,이를 생략하는 것이다. 이로 인해 가수에 1비트를 더 사용할 수 있다.

> **정규화란?**

<hr>
2진수를 1.xxxx * 2^n 꼴로 변환하는 것이다.
예를 들면, 111.101을 1.11101 * 2^2로 변환하는 것
즉, 지수 부분을 1자리 수만 남기고 n만큼 이동시킨다.
그래서 소수점 부분이 옮겨가기에 부동소수점이라 불리는 것이 아닌가 싶다.

정규화를 할 때에 정수 부분에 1만 남겨두는 이유는 이진수에서 가장 앞자리는 항상 1이 되기 때문에 이를 빼고 바로 바로 뒤에 오는 수부터 23 비트를 채우게 되면 결과적으로 24 비트만큼 채우는 효과를 가져오기 때문이다.

<br>

> **편향값 활용**

<hr>
기본정밀도(32비트)는 아래와 같이 표현된다.

![image](https://user-images.githubusercontent.com/91880235/167650897-cfd17323-6061-4a16-aaeb-4416773742b8.png)

부호 비트는 고정소수점에서와 마찬가지로 0이면 양수, 1이면 음수를 의미한다.

23자리 가수부는 정규화 결과 소수점 오른쪽에 있는 숫자들을 왼쪽부터 그대로 넣으면 된다. 남는 자리는 0으로 채운다.

(참고: 소수점 왼쪽은 정규화를 하면 무조건 1이기 때문에 신경쓰지 않고 표현도 안 하는데, 이 1을 hidden bit라고 부르기도 한다)

남은건 8자리짜리 지수부인데, 일단 '지수' 부라는 이름으로 봤을 땐 2^n에서 n에 해당하는 수, 그러니까 예시에서는 2를 2진수로 바꾼 '10' 을 넣으면 될 것 같다.

근데 IEEE 표준에 따르면 저 부분에는 지수를 그대로 박아넣는게 아니라, 'bias' 라고 하는 지정된 숫자를 더한 다음 넣어야 한다.

IEEE 표준에서 32비트를 쓰는 경우 bias는 127이라고 규정하고 있다. 따라서 2 + 127 = 129를 2진수로 바꾼 10000001이 들어간다.

결론적으로 7.625는 컴퓨터에서 아래와 같이 저장된다.

![image](https://user-images.githubusercontent.com/91880235/167650447-c5bb2c8b-6cdb-43d6-9cfd-929158d2071f.png)

이 bias라는 값을 왜 쓰냐면, 지수가 음수가 될 수도 있어서 그렇다.

예를 들면 0.000101이라는 이진수가 있다 치자. 정규화에 대해서 설명할 때 정수부를 1로 만들어야 한다고 했다. 그러니까 왼쪽이 아니라 오른쪽으로 소수점을 밀어서 1.01 \* 2^-4가 된다.

만약에 bias가 없어서 위에서 2를 그냥 00000010으로 저장했다고 생각해보자. -4는 어떻게 저장할래?

부호 비트는 지수의 부호를 뜻하는게 아니라 전체 숫자의 부호를 뜻하는 거라서 이거랑 상관이 없다.

그렇다고 지수용 부호 비트를 하나 더 만들자니 이것대로 복잡하다. 그래서 8자리를 가지고 음수랑 양수를 둘 다 표현하자니, (10진수 기준으로) 0~127 구간은 음수, 128~255 구간은 양수를 표현하도록 만든 것이다.

double, 그러니까 64비트 체계에서는 지수부가 11비트, 가수부가 52비트다. 지수부가 2^11 즉 2048개의 수를 표현할 수 있으므로 0~1023 구간은 음수, 1024~2047 구간은 양수 지수를 의미하며 bias는 1023이 된다.

이와 같은 부동소수점 표현 방식은 위에서 살펴본 고정소수점 표현 방식에 비해서 비트 수 대비 표현 가능한 수의 범위와 정밀도 측면에서 보다 우위에 있기 때문에, 정규화니 bias니 하는 복잡한 과정이 들어감에도 불구하고 현재 대부분의 컴퓨터 시스템에서 부동소수점을 이용해 실수를 표현하고 있다.

### <예시 자료>

![image](https://user-images.githubusercontent.com/91880235/167653531-700475e1-4328-4340-81e5-b7b043455696.png)

![image](https://user-images.githubusercontent.com/91880235/167653565-190fb2fd-0135-43ce-bc6e-0a770038567e.png)

# Reference

- [컴퓨터에서의 실수 표현: 고정소수점 vs 부동소수점](https://gsmesie692.tistory.com/94)
- [부동소수점 방식 표현](https://codetorial.net/articles/floating_point.html)
- [[ 컴퓨터 구조 기초 ] IEEE 754 표준 Interchange Formats와 정규화(Normalize)](https://blog.naver.com/r00tdr4g0n/222009363042)
- [[컴퓨터과학/C언어] IEEE 754 double 형 변환 (64bit)](https://backstreet-programmer.tistory.com/140)

# 7. 2진 코드화한 10진수 시스템

2진 코드화한 10진수(BCD, binary-coded demical)는 4비트를 사용해 10진 숫자를 하나 표현한다.
예를 들어 12를 2진수로 표현하면 1100이지만 BCD로 표현하면 0001 0010이다.
여기서 0001은 십의 자리에 있는 1을, 0010은 일의 자리에 있는 2를 표현한다.
10진수를 사용하는 우리에게는 이런 표현이 훨씬 더 익숙하고 편한 방식이다.
허나, 이런 시스템은 더 이상 주류로 남아있지 않다.(사용 가능한 비트 중 37.5%를 낭비하기 때문) 그러나 이에 대해 알아두면 좋다.
특히 컴퓨터와 상호작용하는 장치 중에서 디스플레이나 가속도 센서 등이 BCD를 사용하는 경우가 있기 때문!

# 8. 2진수를 다루는 쉬운 방법

2진수를 보다보면 눈이 아프다! 그래서 읽기 쉬운 방법을 몇 개 고안 해냈다.

> ### 8진 표현법
>
> 눈에 좋은 표현법 중 하나는 8진 표현법이다. '8진'이라는 말은 밑이 8이라는 뜻으로, 2진수 비트들을 3개씩 그룹으로 묶는 아이디어다.

100101110001010100을 8진 표현법은 4(100) 5(101) 6(110) 1(001) 2(010) 4(100) -> 456124로 바꿀 수 있다.

<br>
<br>

> ### 16진 표현법
>
> 8진 표현법이 아직 쓰이긴 하지만 예전처럼 널리 쓰이진 않는다. 대신 16진 표현법이 쓰이고 있다. 그 이유는 요즘 컴퓨터 내부가 8비트의 배수를 사용해 만들어지기 때문. 8의 배수는 4(16진수 한 자리의 수)로 균일하게 나눠지지만 3(8진수 한 자리의 비트 수)으로는 균일하게 나눠지지 않는다.

16진수 숫자를 표현하려면 9까지의 숫자로는 모두 채울 수 없기 때문에 10~15의 숫자는 abcdef라는 기호가 표현한다.

11010011111111000001이라는 숫자는 d(1101) 3(0011) f(1111) c(1100) 1(0001) -> d3fc1로 바꿀 수 있다.

> ### 프로그래밍 언어의 진법 표기법
>
> 수를 변환하는 방법은 어떻게 알 수 있을까? 예를 들어 10이라는 수가 2진수라면 10진수 2이고, 8진수라면 10진수 8이며, 10진수라면 10이고, 16진수라면 10진수 16이다. 수학책에서는 아래첨자로 각 진법을 구분한다. 하지만 아래 첨자는 컴퓨터 키보드로 입력하기 불편하다.
> 여러 프로그래밍 언어에서는 다음과 같은 표기법을 따른다.

- 0으로 시작하는 숫자는 8진 숫자다. 예를 들어 017은 8진수이며 값은 10진수로 15다.
- 1부터 9사이의 숫자로 시작하는 숫자는 10진수다. 예를 들어 123은 10진수다.
- 0x가 앞에 붙은(접두사) 숫자는 16진수다. 예를 들어, 0x12f는 16진수이며 값은 10진수 303이다.
- C++같은 몇몇 언어는 0b라는 접두사를 사용해 2진수를 표현한다.

## 9. 비트 그룹의 이름

컴퓨터는 제대로 조직화되지 않은 비트들로 이루어지지 않는다. 즉 컴퓨터가 사용할 비트의 개수와 비트의 조직을 결정해 컴퓨터가 유용하게 사용할 수 있도록 처리해야한다.

<br />

### 비트의 조직화

비트는 너무 작아 기본 단위로 사용하려면 유용성이 떨어진다. => 더 큰 덩어리로 조직화

- **허니웰(HoneyWell) 시리즈 컴퓨터**

  - 36비트 덩어리를 기본 조직으로 함
  - 이를 18비트, 9비트, 6비트 덩어리로 나눠서 사용하거나 두 덩어리를 묶어서 72비트 덩어리로 사용

- **DEC PDP-8**

  - 12비트 덩어리 사용
    <br />

점점 시간이 지남에 따라 8비트 덩어리(**바이트**)가 기본 단위로 쓰이기 시작했다.

- **최근 널리 쓰이는 비트의 묶음**
  | 이름 | 비트 개수 |
  | ---------------------- | --------- |
  | 니블(nibble) | 4 |
  | 바이트(byte) | 8 |
  | 하프 워드(half word) | 16 |
  | 워드(word) | 32 |
  | 더블 워드(double word) | 64 |

  - **워드**: 각 컴퓨터가 설계상 자연스럽게 사용할 수 있는 비트의 묶음 (가장 빠르게 처리할 수 있는 가장 큰 덩어리)
    - C, C++ 등의 언어에서 integer로 선언한 변수가 자연스러운 크기의 2진수를 표현한다.
    - 이 외에도 몇가지 정해진 비트 크기의 변수를 선언할 수 있다.

<br />

### 큰 수를 가리키기 위한 표준 용어

| 이름                 | 크기 | 단위      |
| -------------------- | ---- | --------- |
| 킬로바이트(kilobyte) | 2^10 |
| 메가바이트(megabyte) | 2^20 | M 또는 MB |
| 기가바이트(gigabyte) | 2^30 | G 또는 GB |
| 테라바이트(terabyte) | 2^40 | T 또는 TB |

위와 같은 단위들은 밑을 2로 사용하고 있지만, 때때로 밑이 10인 용어를 뜻할 때도 있다. (사피어 대 WDC 소송) 이런 일들로 인해 새로운 IEC 표준 접두사가 만들어졌고 이는 아래와 같다.

| 이름      | 크기 |
| --------- | ---- |
| 키비(KiB) | 2^10 |
| 메비(MiB) | 2^20 |
| 기비(GiB) | 2^30 |
| 테비(TiB) | 2^40 |

<br />

# 10. 텍스트 표현

컴퓨터는 비트를 사용해 수, 문자, 기호 등을 표현한다.
텍스트를 표현하는 방법에도 여러가지 방법이 있었고, 그 중 현재에는 **정보 교환을 위한 미국 표준 코드**(ASCII, American Standard Code for information Interchange)을 주로 사용하고 있다.

<br />

### 아스키 코드 (ASCII Code, 1963)

영문 알파벳을 사용하는 대표적인 문자 인코딩.
키보드에 있는 모든 기호에 대해 **7비트** 수 값을 할당했다.
대부분의 문자 인코딩이 아스키에 기초를 두고 있다.

![아스키코드표](https://github.com/RyanKor/book-review/assets/40455392/64bad519-d68d-4743-bedf-2660641e13bb)

- 33개의 출력 불가능한 제어문자
- 공백을 비롯한 95개의 출력 가능한 문자
  총 128개로 이루어져 있다.

<br />

**제어 문자표**
제어할 때 쓰는 코드이다.

![아스키 제어문자](https://github.com/RyanKor/book-review/assets/40455392/4757d231-aae6-45e1-aaca-1754fe0f271e)

이 중 상당수는 통신 제어를 위한 문자이다.
ex) ACK(수신확인): 메시지를 받았음, NAK(반수신확인): 메시지를 받지 못함

### 다른표준의 진화

초기 컴퓨터는 대부분 미국산 아니면 영국산이었다. 그리고 아스키 코드는 영어를 표현하는데 필요한 모든 문자를 포함하고 있어 표준 역할을 오래 해왔다.

그러나 컴퓨터가 널리 쓰이면서 그 밖에 언어를 지원해야 할 필요가 늘어났다.

- **유럽**: 국제 표준화 기구 (ISO)에서 ISO-646, ISO-8859 도입
  - 아스키를 확장해 유럽 언어에 필요한 액센트 기호, 발음 구별 기호를 추가. (128개 => 256개의 문자를 표현)
- **일본**: JISX 0201 - 일본 문자 표현
- **한국**: KS C 5601 등 표준이 생김

이렇게 다양한 표준이 생긴 이유는 비트가 지금보다 더 비싼 시절에 표준이 만들어졌기 때문이다. 문자를 7비트나 8비트에 욱여넣었다.

그 후 비트 가격이 떨어면서 **유니코드** 라는 새로운 표준이 만들어졌다.

## <br />

---

### 유니코드

- **유니코드 1.0**

  - 문자에 16비트 코드 부여
  - 65,546개의 문자를 포함하는 규격
  - 16비트로 모든 문자를 표현할 수 있을 것이라 생각했지만, 포함되지 않은 문자들이 있어 1년만에 개정되었다.

- **유니코드 2.0**
  - 총 1,114,112 개의 문자를 표현할 수 있다. 거의 지구상의 모든 문자를 표현하며, 아직 표현되지 않은 언어를 위해 예비공간을 확보할 수 있는 정도의 규격이다.
  - 현재 세계적으로 사용되는 유니코드는 유니코드 2.0의 체계 아래에서 업데이트 되고 있다.
    <br />

#### 평면(Plane)

유니코드는 백만이 넘는 범위를 가지므로, 적당한 범위로 나뉘어서 관리를 한다. 이 **범위**를 **평면**이라고 부른다. 즉, 코드포인트를 그룹별로 묶은 범위이다.

| 평면           | 범위(16진법)       | 설명                                                                 |
| -------------- | ------------------ | -------------------------------------------------------------------- |
| 0평면(BMP)     | 0x00000 ~ 0x0FFFF  | 기본 다국어 평면. 현대 언어 대부분의 문자 (유니코드 1.0 시점의 범위) |
| 1평면(SMP)     | 0x10000 ~ 0x1FFFF  | 보조 다국어 평면. 고어 및 여러 기호문자와 이모티콘                   |
| 2평면(SIP)     | 0x20000 ~ 0x2FFFF  | 보조 상형문자 평면. 1평면에 포함되지 않은 한중일 통합 한자           |
| 3~13평면       | 0x30000 ~ 0xDFFFF  | 할당되지 않음(예비)                                                  |
| 14평면(SSP)    | 0xE0000 ~ 0xEFFFF  | 보조 특수 목적 평면. 태그 및 제어용 문자                             |
| 15~16평면(PUA) | 0xF0000 ~ 0x10FFFF | 사용자 정의 영역                                                     |

한글은 대부분 BMP에 들어가있다.
유니코드 2.0에서 정의된 1~16평면을 '보충 평면', '아스트랄플레인'이라고 부르기도 한다.

<br >

#### 코드포인트

유니코드는 문자 하나 당 고유한 숫자값을 부여하였다. 이 숫자값을 코드포인트라고 한다.

ex) "a안b녕"

- 각 글자에 대한 코드 포인트는 아래와 같다.

  - a: 0x61
  - 안: 0xC548
  - b: 0x62
  - 녕: 0xB155

<br >

이를 **바이트 단위**로 적어보면 61 C5 48 62 B1 55 이다. (16진법 두자리 숫자는 1바이트)
그러나 이렇게 차례대로 적은 바이트를 보고 문자로 바꾸는 것은 불가능하다.
61 | C5 48 | 62 | B1 55 =>이런 식으로 구분해야 하는데
61 C5 | 48 62 | B1 55 =>이런 식으로 볼 수도 있다. ( ‘懅 ዾ녕’ 이 된다.. )

<br >

#### **코드 유닛**

위와 같이 바이트를 어떻게 잘라서 하나의 코드 포인트로 볼 것인지에 대한 방법에 대해 생각해보자 .

- 1 바이트 단위로 코드 포인트를 인식하면
  - 61 | C5 | 48 | 62 | B1 | 55 => 6글자로 해석 => ‘aÅ0b±7’
- 2 바이트 단위로 코드 포인트를 인식하면 => ‘懅 ዾ녕’

이렇게 바이트를 어떻게 나누냐에 따라 결과가 완전히 달라진다. => 바이트를 나누는 단위: **코드 유닛**
위의 'a안b녕'을 코드 유닛 2바이트로 해서 바르게 표현하면 다음과 같이 된다.

- a: 00 61
- 안: C5 48
- b: 00 62
- 녕: B1 55

  여기서 a, b는 1바이트만 필요한데도 2바이트 코드유닛에 맞추기 위해 00을 삽입했다. 이를 **패딩**이라 부른다.
  코드 유닛의 크기가 클수록 패딩 크기도 커져서 메모리 사용량이 많아진다.

<br />

---

#### 유니코드 변환 형식

아스키 코드는 문자에 7비트를 부여했지만, 컴퓨터는 7비트값을 처리하도록 설계되지 않아 8비트를 사용해 아스키 문자를 저장한다.

또, 유니코드는 문자에 16비트 코드를 부여했지만, 굳이 16비트를 사용해 낭비할만큼 비트가 저렴하지 않아, 한 문자를 8비트로 표현한다. 이렇게 다른 비트 패턴으로 표현하기 위해 사용하는 비트 패턴을 **인코딩**(encoding)이라고 한다.

<br >

#### 유니코드 변환 형식 8비트 (UTF-8, Unicode Transformation Format-8 bit)

- 하위 호환성과 효율성이 좋아 가장 널리 쓰이고 있는 인코딩 방법이다.
- 1~4bytes의 가변길이 인코딩
- 모든 아스키 문자를 8비트로 표현한다. => 7비트를 8비트로 변환하기 때문에 추가 공간이 필요하지 않음
- 아스키 문자가 아닌 경우, 아스키를 받아서 처리하는 프로그램이 깨지지 않는 방법으로 문자를 인코딩 함

<br />

- UTF-8은 문자를 **8비트 덩어리(옥텟)의 시퀀스로 인코딩**한다.
  ![가변 인코딩 방식](https://miro.medium.com/max/1400/1*A6GcpKbbG-u6ps66f_rEjg.png)(출처: 위키피디아)
  <br />
  글자마다 바이트 길이가 다르므로 가변을 구분하기 위해 첫 바이트(MSB)에 길이를 표현해주었다. 1바이트는 0, 2바이트는 110, 3바이트는 1110으로 시작한다. 나머지 바이트는 10으로 시작한다.
  MSB 쪽의 비트 패턴이 겹치지 않아 덩어리의 맨 앞을 식별하기 쉽다.=> 프로그램이 문자 경계를 찾아야 하는 경우 유용하다.

#### 그 외 인코딩 종류

**UTF-16**

- 2~4 바이트를 코드 유닛으로 사용.
- 자바스크립트 경우 인터넷에서 파일을 받을 때는 UTF-8로 받지만, 그 받은 파일을 해석하여 메모리에 적재하는 시점에는 UTF-16으로 처리한다.
- 그 외에도 자바, 윈도우 응용프로그램 작동 시 사용한다.

**UTF-32**

- 4바이트를 코드 유닛으로 사용.
- 유니코드의 코드 포인트 최대 크기는 4바이트이므로, UTF-32를 쓰는 경우 인코딩, 디코딩 과정이 필요없이 코드포인트 그대로 코드 유닛에 표현할 수 있다.
- 많은 패딩이 발생해 용량이 불어나서, 효율적인 메모리 사용을 위해 UTF-8과 UTF-16을 사용하는 것이다.
- 유닉스 시스템의 일부나 인코딩 이슈를 피하고 싶을 때 사용한다.

---

참고

- 아스키코드? 유니코드가 뭐지? https://dodonam.tistory.com/201
- https://developer.mozilla.org/ko/docs/Glossary/Unicode
- 제어문자 사용하기 https://dojang.io/mod/page/view.php?id=63
- 유니코드 #1 https://www.bsidesoft.com/3435
- 유니코드 #2 https://www.bsidesoft.com/3496
- 유니코드 #3 https://www.bsidesoft.com/3526
- https://miaow-miaow.tistory.com/37

# 11. 문자를 사용한 수 표현

<br/>

### QP 인코딩 ?

<br/>

📍 QP 인코딩이란

- Quoted-Printable encoding
- 아스키 코드(7비트)는 놔두고 나머지 8비트 문자만을 인코딩하는 방식
- 1바이트를 표현하기 위해 3바이트를 사용하기 때문에 아주 비효율적임

<br/>

📍 QP 인코딩 등장 배경

- 8비트 데이터를 7비트 데이터만 지원하는 통신 경로를 통해 송수신하기위해 등장
- 전자 우편 첨부를 처리하기 위해 만들어짐

<br/>

### 베이스64 인코딩 ?

<br/>

📍 base64 인코딩이란

- 3바이트 데이터를 4문자로 표현하는 방식
- 현재 전자 우편 첨부파일 전송에 많이 사용됨
- 보통 1글자를 표현할 때 8비트를 사용하는데, base64는 6비트를 사용해, 효율이 좋음

<br/>

📍 base64 등장 배경

- QP 인코딩방식의 비효율성을 개선하기 위해 등장

<br/>

### URL 인코딩 ?

<br/>

📍 URL 인코딩이란

- percent-encoding(퍼센트 인코딩) 이라고도 부름
- % 뒤에 어떤 문자의 16진 표현을 덧붙이는 방식으로 문자를 인코딩

<br/>

📍 URL 인코딩 등장 배경

- 아스키 코드로 표현할 수 없는, 혹은 표현이 왜곡될만한 문자의 형태를 바로잡기 위해 등장
- 아스키 코드로 URL을 표현했을 때 생길 수 있는 보안 문제 방지 목적

<br/>

### 참고 자료

- [같은 단원을 정리한 블로그1](https://velog.io/@dzpro0327/%EB%AC%B8%EC%9E%90%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%88%98-%ED%91%9C%ED%98%84)

# 12. 색을 표현하는 방법

<br/>

현대에는 24비트를 사용해 약 1천만의 제곱수에 해당하는 색을 표현할 수 있다.  
`24비트`는 `8비트`씩 나뉘어져 3가지 주요 색상(`RGB`)으로 표현한다.  
그러나 현대 컴퓨터는 24비트 단위로 계산을 수행하도록 설계되지 않았기에, `32비트(워드)`에 색을 넣어 처리한다.  
→ 이렇게 되면 사용하지 않는 `8비트`가 남는데, 이는 앞으로 `투명도`를 표현하는 데 사용된다.

<br/>

### 투명도

<br/>

📍 합성 계산법 compositing algebra

- 각 픽셀에 α(투명도)값을 주는 계산법
- α는 1 ~ 255 사이의 값으로 지정되고,
  1에 가까울수록 색이 투명하게 표현되고
  255에 가까울수록 색이 선명하게 표현됨

<br/>

### 색 인코딩

<br/>

📍 16진 트리플렛 hex triplet

- `#` 뒤에 6자리 16진 숫자를 추가해 표현 ex. #000000(검정), #ffffff(하양)
- 텍스트처럼 색을 읽을 수 있어 편리함
